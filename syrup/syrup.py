from flask_cors import CORS
from flask import Flask, request, jsonify
import torch
import sentencepiece as spm
from model import GPT, GPTConfig  # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏£‡∏ô

app = Flask(__name__)
CORS(app)  # ‚úÖ ‡πÄ‡∏õ‡∏¥‡∏î CORS ‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å origin

# === ‡πÇ‡∏´‡∏•‡∏î Tokenizer ===
sp = spm.SentencePieceProcessor()
sp.load("thai_health_tokenizer.model")  # ‡∏ä‡∏∑‡πà‡∏≠ tokenizer ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ

# === ‡πÇ‡∏´‡∏•‡∏î Checkpoint ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ===
checkpoint = torch.load("ckpt.pt", map_location="cpu", weights_only=True)
model_args = checkpoint["model_args"]
model = GPT(GPTConfig(**model_args))
model.load_state_dict(checkpoint["model"])
model.eval()  # ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• (inference)

# === ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö encode ‡πÅ‡∏•‡∏∞ decode ===
def encode(text):
    return sp.encode(text, out_type=int)

def decode(token_ids):
    return sp.decode(token_ids)

@app.route("/chat", methods=["POST"])
def chat():
    try:
        print("üì• ‡∏£‡∏±‡∏ö request POST /chat")

        data = request.get_json(force=True)
        print("üì¶ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö:", data)

        user_input = data.get("message", "")
        print("üü¶ user_input:", user_input)

        input_ids = encode(user_input)
        print("üüß token_ids:", input_ids)

        x = torch.tensor([input_ids], dtype=torch.long)
        with torch.no_grad():
            y = model.generate(x, max_new_tokens=50)[0].tolist()
        response_ids = y[len(input_ids):]
        output = decode(response_ids)
        print("output:", output)

        return jsonify({"response": output})

    except Exception as e:
        print("üî• ERROR:", str(e))
        return jsonify({"response": "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏Ñ‡πà‡∏∞ ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"}), 500




# === ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå ===
if __name__ == "__main__":
    app.run(debug=True)
